// @flow

const { Transform } = require('stream');
const crypto = require('crypto');

/**
 * Emitted by DeserializeTransform streams when the time after the last
 * byte in a chunk received exeeds the 'timeout' parameter
 */
class ChunkTimeoutError extends Error {}

/**
 * Emitted by DeserializeTransform streams that are ended while chunks are
 * in progress
 */
class ChunkIncompleteError extends Error {}


/**
 * Ingests data of any size, emits consistently sized chunks containing
 * a 10 byte header used by DeserializeTransform to reconstruct the original
 * stream
 * @constructor
 * @param {Object} [options={ maxChunkSize: 1316 }] - Transform stream options, see {@link https://nodejs.org/api/stream.html#stream_class_stream_transform Node.js documentation} for full documentation
 * @param {number} [options.maxChunkSize=1316] - Maximum size in bytes of emitted chunks, including a 10 byte header.
 */
class SerializeTransform extends Transform {
  declare maxChunkSize: number;

  constructor(options?: transformStreamOptions & { maxChunkSize?: number } = {}) {
    super(options);
    const { maxChunkSize } = options;
    this.maxChunkSize = typeof maxChunkSize === 'number' ? maxChunkSize : 1316;
  }

  _transform(chunk: Buffer | string, encoding: string, callback: (error: ?Error, data: ?(Buffer | string)) => void) {
    // $FlowFixMe
    const buffer = typeof chunk === 'string' ? Buffer.from(chunk, encoding) : chunk;
    const bufferLength = buffer.length;

    // Write a header containing the ID of this buffer to each slice
    const header = Buffer.alloc(6);
    // write buffer id
    crypto.randomBytes(2).copy(header, 0);
    // buffer length
    header.writeUInt32LE(bufferLength, 2);

    // Max slice length + header
    const maxSliceLength = this.maxChunkSize - 10;
    let offset = 0;

    try {
      // Loop through the buffer and emit chunks starting with a
      // header containing the ID and length of the buffer
      while (offset < bufferLength) {
        const sliceLength = offset + maxSliceLength > buffer.length ? buffer.length - offset : maxSliceLength;
        const slice = Buffer.alloc(10 + sliceLength);
        header.copy(slice, 0);
        slice.writeUInt32LE(offset, 6);
        buffer.copy(slice, 10, offset, offset + sliceLength);
        this.push(slice);
        offset += sliceLength;
      }
      callback();
    } catch (error) {
      callback(error);
    }
  }

  _flush(callback: (error: ?Error, data: ?(Buffer | string)) => void) {
    // Chunks are not cached
    callback();
  }
}

/**
 * Ingests consistently sized chunks generated by SerializeTransform
 * and emits the original, larger chunks
 * @constructor
 * @param {Object} [options={ timeout: 5000 }] - Transform stream options, see {@link https://nodejs.org/api/stream.html#stream_class_stream_transform Node.js documentation} for full documentation
 * @param {number} [options.timeout=5000] - Maximum size in bytes of emitted chunks, including a 10 byte header.
 */
class DeserializeTransform extends Transform {
  declare timeout: number;
  declare bufferMap: Map<number, Buffer>;
  declare bytesRemainingMap: Map<number, number>;
  declare timeoutCheckInterval: IntervalID | void;
  declare timeoutMap: Map<number, number>;

  constructor(options?: transformStreamOptions & { timeout?: number } = {}) {
    super(options);
    this.timeout = options.timeout || 5000;
    this.bufferMap = new Map();
    this.timeoutMap = new Map();
    this.bytesRemainingMap = new Map();
  }

  timeoutCheck() {
    const now = Date.now();
    for (const [id, timestamp] of this.timeoutMap) {
      if (timestamp > now) {
        const bytesRemaining = this.bytesRemainingMap.get(id) || 'unknown';
        const error = new ChunkTimeoutError(`DeserializeTransform timeout error for chunk ${id} after ${this.timeout}ms, ${bytesRemaining} bytes remaining`);
        this.emit('error', error);
      }
    }
  }

  _transform(chunk: Buffer | string, encoding: string, callback: (error: ?Error, data: ?(Buffer | string)) => void) {
    // $FlowFixMe
    const slice = typeof chunk === 'string' ? Buffer.from(chunk, encoding) : chunk;
    // Length of the slice without the header
    const sliceLength = slice.length - 10;
    // ID of the original buffer
    const id = slice.readUInt16LE(0);
    // Length of the original buffer
    const bufferLength = slice.readUInt32LE(2);
    // Offset of this slice
    const offset = slice.readUInt32LE(6);
    // Buffer to be filled in with slice data
    let buffer = this.bufferMap.get(id);
    if (typeof buffer === 'undefined') {
      buffer = Buffer.alloc(bufferLength);
      this.bufferMap.set(id, buffer);
      if (typeof this.timeoutCheckInterval === 'undefined') {
        this.timeoutCheckInterval = setInterval(this.timeoutCheck.bind(this), this.timeout / 2);
      }
    }
    // Copy slice data to the buffer
    slice.copy(buffer, offset, 10);
    // Calculate remaining bytes
    const bytesRemaining = (this.bytesRemainingMap.get(id) || bufferLength) - sliceLength;
    // If slices remain callback and return
    if (bytesRemaining > 0) {
      this.bytesRemainingMap.set(id, bytesRemaining);
      this.timeoutMap.set(id, Date.now() + this.timeout);
      callback();
      return;
    }
    // Emit the buffer and cleanup
    callback(undefined, buffer);
    this.bytesRemainingMap.delete(id);
    this.bufferMap.delete(id);
    this.timeoutMap.delete(id);
    if (this.timeoutMap.size === 0) {
      clearInterval(this.timeoutCheckInterval);
      delete this.timeoutCheckInterval;
    }
  }

  _flush(callback: (error: ?Error, data: ?(Buffer | string)) => void) {
    if (this.bufferMap.size === 0) {
      callback();
      return;
    }
    const error = new ChunkIncompleteError(`Unable to flush DeserializeTransform, ${this.bufferMap.size} ${this.bufferMap.size > 1 ? 'chunks are' : 'chunk is'} pending`);
    callback(error);
  }
}

module.exports.ChunkTimeoutError = ChunkTimeoutError;
module.exports.ChunkIncompleteError = ChunkIncompleteError;
module.exports.SerializeTransform = SerializeTransform;
module.exports.DeserializeTransform = DeserializeTransform;

